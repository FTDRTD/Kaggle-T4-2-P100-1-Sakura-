{"metadata":{"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":30762,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"[![Kaggle](https://kaggle.com/static/images/open-in-kaggle.svg)](https://kaggle.com/kernels/welcome?src=https://github.com/Isotr0py/SakuraLLM-Notebooks/blob/main/Sakura-13B-Galgame-Kaggle-ChatGPTQ.ipynb)","metadata":{}},{"cell_type":"code","source":"!git clone https://github.com/SakuraLLM/Sakura-13B-Galgame.git\n%cd Sakura-13B-Galgame\n!pip install -q -U torch torchvision torchaudio xformers --index-url https://download.pytorch.org/whl/cu121\n!pip install -q auto-gptq\n!pip install -q -r requirements.txt\n!pip install -q pyngrok\n\n# install localtunnel\n!npm install -g localtunnel","metadata":{"execution":{"iopub.status.busy":"2024-09-09T01:10:41.724343Z","iopub.execute_input":"2024-09-09T01:10:41.724726Z","iopub.status.idle":"2024-09-09T01:15:00.749219Z","shell.execute_reply.started":"2024-09-09T01:10:41.724690Z","shell.execute_reply":"2024-09-09T01:15:00.748199Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"Cloning into 'Sakura-13B-Galgame'...\nremote: Enumerating objects: 981, done.\u001b[K\nremote: Counting objects: 100% (439/439), done.\u001b[K\nremote: Compressing objects: 100% (226/226), done.\u001b[K\nremote: Total 981 (delta 298), reused 286 (delta 210), pack-reused 542 (from 1)\u001b[K\nReceiving objects: 100% (981/981), 297.65 KiB | 3.31 MiB/s, done.\nResolving deltas: 100% (538/538), done.\n/kaggle/working/Sakura-13B-Galgame/Sakura-13B-Galgame\n\u001b[K\u001b[?25hm#################\u001b[0m\u001b[100;90m.\u001b[0m] / reify:yargs-parser: \u001b[32;40mhttp\u001b[0m \u001b[35mfetch\u001b[0m GET 200 https://registry.\u001b[0m\u001b[K\nadded 22 packages in 2s\n\n3 packages are looking for funding\n  run `npm fund` for details\n\u001b[37;40mnpm\u001b[0m \u001b[0m\u001b[36;40mnotice\u001b[0m\u001b[35m\u001b[0m \n\u001b[0m\u001b[37;40mnpm\u001b[0m \u001b[0m\u001b[36;40mnotice\u001b[0m\u001b[35m\u001b[0m New \u001b[33mminor\u001b[39m version of npm available! \u001b[31m10.5.0\u001b[39m -> \u001b[32m10.8.3\u001b[39m\n\u001b[0m\u001b[37;40mnpm\u001b[0m \u001b[0m\u001b[36;40mnotice\u001b[0m\u001b[35m\u001b[0m Changelog: \u001b[36mhttps://github.com/npm/cli/releases/tag/v10.8.3\u001b[39m\n\u001b[0m\u001b[37;40mnpm\u001b[0m \u001b[0m\u001b[36;40mnotice\u001b[0m\u001b[35m\u001b[0m Run \u001b[32mnpm install -g npm@10.8.3\u001b[39m to update!\n\u001b[0m\u001b[37;40mnpm\u001b[0m \u001b[0m\u001b[36;40mnotice\u001b[0m\u001b[35m\u001b[0m \n\u001b[0m","output_type":"stream"}]},{"cell_type":"code","source":"# ngrokToken留空则使用localtunnel进行内网穿透\nngrokToken = \"2jGyaSU2qDyRYae0kRsUQ7U6PJ4_68FBmrHxsyRe2zg1HV3yU\"\nuse_pinggy = True\nMODEL = \"SakuraLLM/Sakura-13B-LNovel-v0_8-8bit\"\n\n\nif ngrokToken:\n    from pyngrok import conf, ngrok\n    conf.get_default().auth_token = ngrokToken\n    conf.get_default().monitor_thread = False\n    ssh_tunnels = ngrok.get_tunnels(conf.get_default())\n    if len(ssh_tunnels) == 0:\n        ssh_tunnel = ngrok.connect(5000)\n        print('address：'+ssh_tunnel.public_url)\n    else:\n        print('address：'+ssh_tunnels[0].public_url)\nelif use_pinggy:\n    import subprocess\n    import threading\n    def start_pinggy(port):\n        cmd = f\"ssh -o UserKnownHostsFile=/dev/null -o StrictHostKeyChecking=no -p 443 -R0:localhost:{port} a.pinggy.io\"\n        p = subprocess.Popen(cmd.split(\" \"), stdout=subprocess.PIPE)\n        for line in p.stdout:\n            print(line.decode(), end='')\n    threading.Thread(target=start_pinggy, daemon=True, args=(5000,)).start()\nelse:\n    import subprocess\n    import threading\n    def start_localtunnel(port):\n        p = subprocess.Popen([\"lt\", \"--port\", f\"{port}\"], stdout=subprocess.PIPE)\n        for line in p.stdout:\n            print(line.decode(), end='')\n    threading.Thread(target=start_localtunnel, daemon=True, args=(5000,)).start()\n\nwhile True:\n    !python server.py \\\n  --model_name_or_path $MODEL \\\n  --use_gptq_model \\\n  --model_version 0.8 \\\n  --trust_remote_code \\\n  --no-auth\n","metadata":{"execution":{"iopub.status.busy":"2024-09-09T02:00:34.463011Z","iopub.execute_input":"2024-09-09T02:00:34.463359Z"},"trusted":true},"execution_count":null,"outputs":[{"name":"stdout","text":"address：https://84bb-34-28-70-138.ngrok-free.app\n\u001b[32m2024-09-09 02:00:43\u001b[0m \u001b[35m12695202b7b4\u001b[0m \u001b[34m__main__[243]\u001b[0m \u001b[1;30mWARNING\u001b[0m \u001b[33mAuth is disabled!\u001b[0m\n\u001b[32m2024-09-09 02:00:43\u001b[0m \u001b[35m12695202b7b4\u001b[0m \u001b[34m__main__[243]\u001b[0m \u001b[1;30mINFO\u001b[0m Current server config: Server(listen: 127.0.0.1:5000, auth: None:None)\n\u001b[32m2024-09-09 02:00:43\u001b[0m \u001b[35m12695202b7b4\u001b[0m \u001b[34m__main__[243]\u001b[0m \u001b[1;30mINFO\u001b[0m Current model config: SakuraModelConfig(model_name_or_path='SakuraLLM/Sakura-13B-LNovel-v0_8-8bit', use_gptq_model=True, use_awq_model=False, trust_remote_code=True, text_length=512, llama=False, llama_cpp=False, use_gpu=False, n_gpu_layers=0, vllm=False, enforce_eager=False, tensor_parallel_size=1, gpu_memory_utilization=0.9, ollama=False, model_name=None, model_quant=None, model_version='0.8')\n\u001b[32m2024-09-09 02:00:52\u001b[0m \u001b[35m12695202b7b4\u001b[0m \u001b[34mnumexpr.utils[243]\u001b[0m \u001b[1;30mINFO\u001b[0m NumExpr defaulting to 4 threads.\n/opt/conda/lib/python3.10/site-packages/transformers/utils/generic.py:311: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.\n  torch.utils._pytree._register_pytree_node(\n\u001b[32m2024-09-09 02:00:53\u001b[0m \u001b[35m12695202b7b4\u001b[0m \u001b[34mutils.model[243]\u001b[0m \u001b[1;30mINFO\u001b[0m loading model ...\n/opt/conda/lib/python3.10/site-packages/huggingface_hub/file_download.py:1150: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n  warnings.warn(\n\u001b[32m2024-09-09 02:00:53\u001b[0m \u001b[35m12695202b7b4\u001b[0m \u001b[34mdatasets[243]\u001b[0m \u001b[1;30mINFO\u001b[0m PyTorch version 2.4.0+cu121 available.\n\u001b[32m2024-09-09 02:00:53\u001b[0m \u001b[35m12695202b7b4\u001b[0m \u001b[34mdatasets[243]\u001b[0m \u001b[1;30mINFO\u001b[0m Polars version 1.5.0 available.\n\u001b[32m2024-09-09 02:00:53\u001b[0m \u001b[35m12695202b7b4\u001b[0m \u001b[34mdatasets[243]\u001b[0m \u001b[1;30mINFO\u001b[0m TensorFlow version 2.16.1 available.\n\u001b[32m2024-09-09 02:00:53\u001b[0m \u001b[35m12695202b7b4\u001b[0m \u001b[34mdatasets[243]\u001b[0m \u001b[1;30mINFO\u001b[0m JAX version 0.4.26 available.\n\u001b[32m2024-09-09 02:00:53\u001b[0m \u001b[35m12695202b7b4\u001b[0m \u001b[34mdatasets[243]\u001b[0m \u001b[1;30mINFO\u001b[0m Apache Beam version 2.46.0 available.\n/opt/conda/lib/python3.10/site-packages/auto_gptq/nn_modules/triton_utils/kernels.py:411: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.\n  def forward(ctx, input, qweight, scales, qzeros, g_idx, bits, maxq):\n/opt/conda/lib/python3.10/site-packages/auto_gptq/nn_modules/triton_utils/kernels.py:419: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.\n  def backward(ctx, grad_output):\n/opt/conda/lib/python3.10/site-packages/auto_gptq/nn_modules/triton_utils/kernels.py:461: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.\n  @custom_fwd(cast_inputs=torch.float16)\n\u001b[32m2024-09-09 02:00:55\u001b[0m \u001b[35m12695202b7b4\u001b[0m \u001b[34mauto_gptq.nn_modules.qlinear.qlinear_cuda[243]\u001b[0m \u001b[1;30mWARNING\u001b[0m \u001b[33mCUDA extension not installed.\u001b[0m\n\u001b[32m2024-09-09 02:00:55\u001b[0m \u001b[35m12695202b7b4\u001b[0m \u001b[34mauto_gptq.nn_modules.qlinear.qlinear_cuda_old[243]\u001b[0m \u001b[1;30mWARNING\u001b[0m \u001b[33mCUDA extension not installed.\u001b[0m\n/opt/conda/lib/python3.10/site-packages/huggingface_hub/file_download.py:1150: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n  warnings.warn(\nWARNING - Exllamav2 kernel is not installed, reset disable_exllamav2 to True. This may because you installed auto_gptq using a pre-build wheel on Windows, in which exllama_kernels are not compiled. To use exllama_kernels to further speedup inference, you can re-install auto_gptq from source.\n\u001b[32m2024-09-09 02:00:55\u001b[0m \u001b[35m12695202b7b4\u001b[0m \u001b[34mauto_gptq.modeling._base[243]\u001b[0m \u001b[1;30mWARNING\u001b[0m \u001b[33mExllamav2 kernel is not installed, reset disable_exllamav2 to True. This may because you installed auto_gptq using a pre-build wheel on Windows, in which exllama_kernels are not compiled. To use exllama_kernels to further speedup inference, you can re-install auto_gptq from source.\u001b[0m\nWARNING - CUDA kernels for auto_gptq are not installed, this will result in very slow inference speed. This may because:\n1. You disabled CUDA extensions compilation by setting BUILD_CUDA_EXT=0 when install auto_gptq from source.\n2. You are using pytorch without CUDA support.\n3. CUDA and nvcc are not installed in your device.\n\u001b[32m2024-09-09 02:00:55\u001b[0m \u001b[35m12695202b7b4\u001b[0m \u001b[34mauto_gptq.modeling._base[243]\u001b[0m \u001b[1;30mWARNING\u001b[0m \u001b[33mCUDA kernels for auto_gptq are not installed, this will result in very slow inference speed. This may because:\n1. You disabled CUDA extensions compilation by setting BUILD_CUDA_EXT=0 when install auto_gptq from source.\n2. You are using pytorch without CUDA support.\n3. CUDA and nvcc are not installed in your device.\u001b[0m\n/opt/conda/lib/python3.10/site-packages/xformers/ops/fmha/flash.py:211: FutureWarning: `torch.library.impl_abstract` was renamed to `torch.library.register_fake`. Please use that instead; we will remove `torch.library.impl_abstract` in a future version of PyTorch.\n  @torch.library.impl_abstract(\"xformers_flash::flash_fwd\")\n/opt/conda/lib/python3.10/site-packages/xformers/ops/fmha/flash.py:344: FutureWarning: `torch.library.impl_abstract` was renamed to `torch.library.register_fake`. Please use that instead; we will remove `torch.library.impl_abstract` in a future version of PyTorch.\n  @torch.library.impl_abstract(\"xformers_flash::flash_bwd\")\n/opt/conda/lib/python3.10/site-packages/accelerate/utils/modeling.py:1532: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n  return torch.load(checkpoint_file, map_location=torch.device(\"cpu\"))\nWARNING - can't get model's sequence length from model config, will set to 4096.\n\u001b[32m2024-09-09 02:02:02\u001b[0m \u001b[35m12695202b7b4\u001b[0m \u001b[34mauto_gptq.modeling._base[243]\u001b[0m \u001b[1;30mWARNING\u001b[0m \u001b[33mcan't get model's sequence length from model config, will set to 4096.\u001b[0m\n\u001b[32m2024-09-09 02:02:11\u001b[0m \u001b[35m12695202b7b4\u001b[0m \u001b[34mroot[243]\u001b[0m \u001b[1;30mWARNING\u001b[0m \u001b[33mmodel output is not correct, please check the loaded model\u001b[0m\n\u001b[32m2024-09-09 02:02:11\u001b[0m \u001b[35m12695202b7b4\u001b[0m \u001b[34mroot[243]\u001b[0m \u001b[1;30mWARNING\u001b[0m \u001b[33minput: <reserved_106>将下面的日文文本翻译成中文：やる気マンゴスキン<reserved_107>\u001b[0m\n\u001b[32m2024-09-09 02:02:11\u001b[0m \u001b[35m12695202b7b4\u001b[0m \u001b[34mroot[243]\u001b[0m \u001b[1;30mWARNING\u001b[0m \u001b[33mground_truth: 干劲Mangoskin\u001b[0m\n\u001b[32m2024-09-09 02:02:11\u001b[0m \u001b[35m12695202b7b4\u001b[0m \u001b[34mroot[243]\u001b[0m \u001b[1;30mWARNING\u001b[0m \u001b[33mcurrent output: prompt_token=20 new_token=3 text='干劲满满' finish_reason='stop'\u001b[0m\n\u001b[32m2024-09-09 02:02:11\u001b[0m \u001b[35m12695202b7b4\u001b[0m \u001b[34m__main__[243]\u001b[0m \u001b[1;30mINFO\u001b[0m Server will run at http://127.0.0.1:5000, preparing...\n[2024-09-09 02:02:11 +0000] [243] [INFO] Running on http://127.0.0.1:5000 (CTRL + C to quit)\n\u001b[32m2024-09-09 02:02:11\u001b[0m \u001b[35m12695202b7b4\u001b[0m \u001b[34mhypercorn.error[243]\u001b[0m \u001b[1;30mINFO\u001b[0m Running on http://127.0.0.1:5000 (CTRL + C to quit)\n","output_type":"stream"}]}]}