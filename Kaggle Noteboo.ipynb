{"metadata":{"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"datasetId":4208491,"sourceId":7261583,"sourceType":"datasetVersion"}],"dockerImageVersionId":30627,"isGpuEnabled":true,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.12"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"code","source":"!git clone https://github.com/SakuraLLM/Sakura-13B-Galgame.git\n%cd Sakura-13B-Galgame\n!pip install -q -U torch torchvision torchaudio xformers --index-url https://download.pytorch.org/whl/cu121\n!pip install -q auto-gptq\n!pip install -q -r requirements.txt\n!pip install -q pyngrok\n\n# install localtunnel\n!npm install -g localtunnel","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# ngrokToken留空则使用localtunnel进行内网穿透\nngrokToken = \"2jGyaSU2qDyRYae0kRsUQ7U6PJ4_68FBmrHxsyRe2zg1HV3yU\"\nuse_pinggy = True\nMODEL = \"SakuraLLM/Sakura-13B-LNovel-v0_8-8bit\"\n\n\nif ngrokToken:\n    from pyngrok import conf, ngrok\n    conf.get_default().auth_token = ngrokToken\n    conf.get_default().monitor_thread = False\n    ssh_tunnels = ngrok.get_tunnels(conf.get_default())\n    if len(ssh_tunnels) == 0:\n        ssh_tunnel = ngrok.connect(5000)\n        print('address：'+ssh_tunnel.public_url)\n    else:\n        print('address：'+ssh_tunnels[0].public_url)\nelif use_pinggy:\n    import subprocess\n    import threading\n    def start_pinggy(port):\n        cmd = f\"ssh -o UserKnownHostsFile=/dev/null -o StrictHostKeyChecking=no -p 443 -R0:localhost:{port} a.pinggy.io\"\n        p = subprocess.Popen(cmd.split(\" \"), stdout=subprocess.PIPE)\n        for line in p.stdout:\n            print(line.decode(), end='')\n    threading.Thread(target=start_pinggy, daemon=True, args=(5000,)).start()\nelse:\n    import subprocess\n    import threading\n    def start_localtunnel(port):\n        p = subprocess.Popen([\"lt\", \"--port\", f\"{port}\"], stdout=subprocess.PIPE)\n        for line in p.stdout:\n            print(line.decode(), end='')\n    threading.Thread(target=start_localtunnel, daemon=True, args=(5000,)).start()\n\nwhile True:\n    !python server.py \\\n  --model_name_or_path $MODEL \\\n  --use_gptq_model \\\n  --model_version 0.8 \\\n  --trust_remote_code \\\n  --no-auth","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}