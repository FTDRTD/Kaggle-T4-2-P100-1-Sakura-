{"metadata":{"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":30762,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"[![Kaggle](https://kaggle.com/static/images/open-in-kaggle.svg)](https://kaggle.com/kernels/welcome?src=https://github.com/Isotr0py/SakuraLLM-Notebooks/blob/main/Sakura-13B-Galgame-Kaggle-ChatGPTQ.ipynb)","metadata":{}},{"cell_type":"code","source":"!git clone https://github.com/SakuraLLM/Sakura-13B-Galgame.git\n%cd Sakura-13B-Galgame\n!pip install -q -U torch torchvision torchaudio xformers --index-url https://download.pytorch.org/whl/cu121\n!pip install -q auto-gptq\n!pip install -q -r requirements.txt\n!pip install -q pyngrok\n\n# install localtunnel\n!npm install -g localtunnel","metadata":{"execution":{"iopub.status.busy":"2024-09-08T15:11:57.304929Z","iopub.execute_input":"2024-09-08T15:11:57.305545Z"},"trusted":true},"execution_count":null,"outputs":[{"name":"stdout","text":"Cloning into 'Sakura-13B-Galgame'...\nremote: Enumerating objects: 981, done.\u001b[K\nremote: Counting objects: 100% (439/439), done.\u001b[K\nremote: Compressing objects: 100% (226/226), done.\u001b[K\nremote: Total 981 (delta 299), reused 286 (delta 210), pack-reused 542 (from 1)\u001b[K\nReceiving objects: 100% (981/981), 297.40 KiB | 9.59 MiB/s, done.\nResolving deltas: 100% (539/539), done.\n/kaggle/working/Sakura-13B-Galgame\n","output_type":"stream"}]},{"cell_type":"code","source":"# ngrokToken留空则使用localtunnel进行内网穿透\nngrokToken = \"2jGyaSU2qDyRYae0kRsUQ7U6PJ4_68FBmrHxsyRe2zg1HV3yU\"\nuse_pinggy = True\nMODEL = \"SakuraLLM/Sakura-13B-LNovel-v0_8-8bit\"\n\n\nif ngrokToken:\n    from pyngrok import conf, ngrok\n    conf.get_default().auth_token = ngrokToken\n    conf.get_default().monitor_thread = False\n    ssh_tunnels = ngrok.get_tunnels(conf.get_default())\n    if len(ssh_tunnels) == 0:\n        ssh_tunnel = ngrok.connect(5000)\n        print('address：'+ssh_tunnel.public_url)\n    else:\n        print('address：'+ssh_tunnels[0].public_url)\nelif use_pinggy:\n    import subprocess\n    import threading\n    def start_pinggy(port):\n        cmd = f\"ssh -o UserKnownHostsFile=/dev/null -o StrictHostKeyChecking=no -p 443 -R0:localhost:{port} a.pinggy.io\"\n        p = subprocess.Popen(cmd.split(\" \"), stdout=subprocess.PIPE)\n        for line in p.stdout:\n            print(line.decode(), end='')\n    threading.Thread(target=start_pinggy, daemon=True, args=(5000,)).start()\nelse:\n    import subprocess\n    import threading\n    def start_localtunnel(port):\n        p = subprocess.Popen([\"lt\", \"--port\", f\"{port}\"], stdout=subprocess.PIPE)\n        for line in p.stdout:\n            print(line.decode(), end='')\n    threading.Thread(target=start_localtunnel, daemon=True, args=(5000,)).start()\n\n!python server.py \\\n  --model_name_or_path $MODEL \\\n  --use_gptq_model \\\n  --model_version 0.8 \\\n  --trust_remote_code \\\n  --no-auth","metadata":{"execution":{"iopub.status.busy":"2024-09-08T14:40:01.273093Z","iopub.execute_input":"2024-09-08T14:40:01.273404Z"},"trusted":true},"execution_count":null,"outputs":[{"name":"stdout","text":"address：https://31a4-35-188-128-107.ngrok-free.app                                                  \n\u001b[32m2024-09-08 14:40:08\u001b[0m \u001b[35m787c2bb17e6d\u001b[0m \u001b[34m__main__[144]\u001b[0m \u001b[1;30mWARNING\u001b[0m \u001b[33mAuth is disabled!\u001b[0m\n\u001b[32m2024-09-08 14:40:08\u001b[0m \u001b[35m787c2bb17e6d\u001b[0m \u001b[34m__main__[144]\u001b[0m \u001b[1;30mINFO\u001b[0m Current server config: Server(listen: 127.0.0.1:5000, auth: None:None)\n\u001b[32m2024-09-08 14:40:08\u001b[0m \u001b[35m787c2bb17e6d\u001b[0m \u001b[34m__main__[144]\u001b[0m \u001b[1;30mINFO\u001b[0m Current model config: SakuraModelConfig(model_name_or_path='SakuraLLM/Sakura-13B-LNovel-v0_8-8bit', use_gptq_model=True, use_awq_model=False, trust_remote_code=True, text_length=512, llama=False, llama_cpp=False, use_gpu=False, n_gpu_layers=0, vllm=False, enforce_eager=False, tensor_parallel_size=1, gpu_memory_utilization=0.9, ollama=False, model_name=None, model_quant=None, model_version='0.8')\n\u001b[32m2024-09-08 14:40:19\u001b[0m \u001b[35m787c2bb17e6d\u001b[0m \u001b[34mnumexpr.utils[144]\u001b[0m \u001b[1;30mINFO\u001b[0m NumExpr defaulting to 4 threads.\n/opt/conda/lib/python3.10/site-packages/transformers/utils/generic.py:311: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.\n  torch.utils._pytree._register_pytree_node(\n\u001b[32m2024-09-08 14:40:20\u001b[0m \u001b[35m787c2bb17e6d\u001b[0m \u001b[34mutils.model[144]\u001b[0m \u001b[1;30mINFO\u001b[0m loading model ...\n/opt/conda/lib/python3.10/site-packages/huggingface_hub/file_download.py:1150: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n  warnings.warn(\ntokenizer_config.json: 100%|███████████████████| 954/954 [00:00<00:00, 7.20MB/s]\ntokenization_baichuan.py: 100%|████████████| 9.03k/9.03k [00:00<00:00, 64.6MB/s]\nA new version of the following files was downloaded from https://huggingface.co/SakuraLLM/Sakura-13B-LNovel-v0_8-8bit:\n- tokenization_baichuan.py\n. Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.\ntokenizer.model: 100%|█████████████████████| 2.00M/2.00M [00:00<00:00, 27.4MB/s]\nspecial_tokens_map.json: 100%|█████████████████| 432/432 [00:00<00:00, 4.24MB/s]\n\u001b[32m2024-09-08 14:40:20\u001b[0m \u001b[35m787c2bb17e6d\u001b[0m \u001b[34mdatasets[144]\u001b[0m \u001b[1;30mINFO\u001b[0m PyTorch version 2.4.0+cu121 available.\n\u001b[32m2024-09-08 14:40:20\u001b[0m \u001b[35m787c2bb17e6d\u001b[0m \u001b[34mdatasets[144]\u001b[0m \u001b[1;30mINFO\u001b[0m Polars version 1.5.0 available.\n\u001b[32m2024-09-08 14:40:20\u001b[0m \u001b[35m787c2bb17e6d\u001b[0m \u001b[34mdatasets[144]\u001b[0m \u001b[1;30mINFO\u001b[0m TensorFlow version 2.16.1 available.\n\u001b[32m2024-09-08 14:40:20\u001b[0m \u001b[35m787c2bb17e6d\u001b[0m \u001b[34mdatasets[144]\u001b[0m \u001b[1;30mINFO\u001b[0m JAX version 0.4.26 available.\n\u001b[32m2024-09-08 14:40:20\u001b[0m \u001b[35m787c2bb17e6d\u001b[0m \u001b[34mdatasets[144]\u001b[0m \u001b[1;30mINFO\u001b[0m Apache Beam version 2.46.0 available.\n/opt/conda/lib/python3.10/site-packages/auto_gptq/nn_modules/triton_utils/kernels.py:411: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.\n  def forward(ctx, input, qweight, scales, qzeros, g_idx, bits, maxq):\n/opt/conda/lib/python3.10/site-packages/auto_gptq/nn_modules/triton_utils/kernels.py:419: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.\n  def backward(ctx, grad_output):\n/opt/conda/lib/python3.10/site-packages/auto_gptq/nn_modules/triton_utils/kernels.py:461: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.\n  @custom_fwd(cast_inputs=torch.float16)\n\u001b[32m2024-09-08 14:40:22\u001b[0m \u001b[35m787c2bb17e6d\u001b[0m \u001b[34mauto_gptq.nn_modules.qlinear.qlinear_cuda[144]\u001b[0m \u001b[1;30mWARNING\u001b[0m \u001b[33mCUDA extension not installed.\u001b[0m\n\u001b[32m2024-09-08 14:40:22\u001b[0m \u001b[35m787c2bb17e6d\u001b[0m \u001b[34mauto_gptq.nn_modules.qlinear.qlinear_cuda_old[144]\u001b[0m \u001b[1;30mWARNING\u001b[0m \u001b[33mCUDA extension not installed.\u001b[0m\n/opt/conda/lib/python3.10/site-packages/huggingface_hub/file_download.py:1150: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n  warnings.warn(\nconfig.json: 100%|█████████████████████████████| 836/836 [00:00<00:00, 7.03MB/s]\nconfiguration_baichuan.py: 100%|███████████| 1.56k/1.56k [00:00<00:00, 13.1MB/s]\nA new version of the following files was downloaded from https://huggingface.co/SakuraLLM/Sakura-13B-LNovel-v0_8-8bit:\n- configuration_baichuan.py\n. Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.\nWARNING - Exllamav2 kernel is not installed, reset disable_exllamav2 to True. This may because you installed auto_gptq using a pre-build wheel on Windows, in which exllama_kernels are not compiled. To use exllama_kernels to further speedup inference, you can re-install auto_gptq from source.\n\u001b[32m2024-09-08 14:40:22\u001b[0m \u001b[35m787c2bb17e6d\u001b[0m \u001b[34mauto_gptq.modeling._base[144]\u001b[0m \u001b[1;30mWARNING\u001b[0m \u001b[33mExllamav2 kernel is not installed, reset disable_exllamav2 to True. This may because you installed auto_gptq using a pre-build wheel on Windows, in which exllama_kernels are not compiled. To use exllama_kernels to further speedup inference, you can re-install auto_gptq from source.\u001b[0m\nWARNING - CUDA kernels for auto_gptq are not installed, this will result in very slow inference speed. This may because:\n1. You disabled CUDA extensions compilation by setting BUILD_CUDA_EXT=0 when install auto_gptq from source.\n2. You are using pytorch without CUDA support.\n3. CUDA and nvcc are not installed in your device.\n\u001b[32m2024-09-08 14:40:22\u001b[0m \u001b[35m787c2bb17e6d\u001b[0m \u001b[34mauto_gptq.modeling._base[144]\u001b[0m \u001b[1;30mWARNING\u001b[0m \u001b[33mCUDA kernels for auto_gptq are not installed, this will result in very slow inference speed. This may because:\n1. You disabled CUDA extensions compilation by setting BUILD_CUDA_EXT=0 when install auto_gptq from source.\n2. You are using pytorch without CUDA support.\n3. CUDA and nvcc are not installed in your device.\u001b[0m\nquantize_config.json: 100%|████████████████████| 211/211 [00:00<00:00, 1.74MB/s]\ngptq_model-8bit-128g.bin: 100%|█████████████| 15.5G/15.5G [02:12<00:00, 117MB/s]\nmodeling_baichuan.py: 100%|████████████████| 32.5k/32.5k [00:00<00:00, 3.56MB/s]\nquantizer.py: 100%|████████████████████████| 9.11k/9.11k [00:00<00:00, 48.1MB/s]\nA new version of the following files was downloaded from https://huggingface.co/SakuraLLM/Sakura-13B-LNovel-v0_8-8bit:\n- quantizer.py\n. Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.\ngeneration_utils.py: 100%|█████████████████| 2.97k/2.97k [00:00<00:00, 19.6MB/s]\nA new version of the following files was downloaded from https://huggingface.co/SakuraLLM/Sakura-13B-LNovel-v0_8-8bit:\n- generation_utils.py\n. Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.\nA new version of the following files was downloaded from https://huggingface.co/SakuraLLM/Sakura-13B-LNovel-v0_8-8bit:\n- modeling_baichuan.py\n- quantizer.py\n- generation_utils.py\n. Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.\n/opt/conda/lib/python3.10/site-packages/xformers/ops/fmha/flash.py:211: FutureWarning: `torch.library.impl_abstract` was renamed to `torch.library.register_fake`. Please use that instead; we will remove `torch.library.impl_abstract` in a future version of PyTorch.\n  @torch.library.impl_abstract(\"xformers_flash::flash_fwd\")\n/opt/conda/lib/python3.10/site-packages/xformers/ops/fmha/flash.py:344: FutureWarning: `torch.library.impl_abstract` was renamed to `torch.library.register_fake`. Please use that instead; we will remove `torch.library.impl_abstract` in a future version of PyTorch.\n  @torch.library.impl_abstract(\"xformers_flash::flash_bwd\")\n/opt/conda/lib/python3.10/site-packages/accelerate/utils/modeling.py:1532: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n  return torch.load(checkpoint_file, map_location=torch.device(\"cpu\"))\nWARNING - can't get model's sequence length from model config, will set to 4096.\n\u001b[32m2024-09-08 14:43:46\u001b[0m \u001b[35m787c2bb17e6d\u001b[0m \u001b[34mauto_gptq.modeling._base[144]\u001b[0m \u001b[1;30mWARNING\u001b[0m \u001b[33mcan't get model's sequence length from model config, will set to 4096.\u001b[0m\n\u001b[32m2024-09-08 14:43:56\u001b[0m \u001b[35m787c2bb17e6d\u001b[0m \u001b[34mroot[144]\u001b[0m \u001b[1;30mWARNING\u001b[0m \u001b[33mmodel output is not correct, please check the loaded model\u001b[0m\n\u001b[32m2024-09-08 14:43:56\u001b[0m \u001b[35m787c2bb17e6d\u001b[0m \u001b[34mroot[144]\u001b[0m \u001b[1;30mWARNING\u001b[0m \u001b[33minput: <reserved_106>将下面的日文文本翻译成中文：やる気マンゴスキン<reserved_107>\u001b[0m\n\u001b[32m2024-09-08 14:43:56\u001b[0m \u001b[35m787c2bb17e6d\u001b[0m \u001b[34mroot[144]\u001b[0m \u001b[1;30mWARNING\u001b[0m \u001b[33mground_truth: 干劲Mangoskin\u001b[0m\n\u001b[32m2024-09-08 14:43:56\u001b[0m \u001b[35m787c2bb17e6d\u001b[0m \u001b[34mroot[144]\u001b[0m \u001b[1;30mWARNING\u001b[0m \u001b[33mcurrent output: prompt_token=20 new_token=3 text='干劲满满' finish_reason='stop'\u001b[0m\n\u001b[32m2024-09-08 14:43:56\u001b[0m \u001b[35m787c2bb17e6d\u001b[0m \u001b[34m__main__[144]\u001b[0m \u001b[1;30mINFO\u001b[0m Server will run at http://127.0.0.1:5000, preparing...\n[2024-09-08 14:43:56 +0000] [144] [INFO] Running on http://127.0.0.1:5000 (CTRL + C to quit)\n\u001b[32m2024-09-08 14:43:56\u001b[0m \u001b[35m787c2bb17e6d\u001b[0m \u001b[34mhypercorn.error[144]\u001b[0m \u001b[1;30mINFO\u001b[0m Running on http://127.0.0.1:5000 (CTRL + C to quit)\n\u001b[32m2024-09-08 14:56:20\u001b[0m \u001b[35m787c2bb17e6d\u001b[0m \u001b[34mapi[144]\u001b[0m \u001b[1;30mINFO\u001b[0m Incoming request api: http://31a4-35-188-128-107.ngrok-free.app/v1/chat/completions\n\u001b[32m2024-09-08 14:56:20\u001b[0m \u001b[35m787c2bb17e6d\u001b[0m \u001b[34mapi[144]\u001b[0m \u001b[1;30mINFO\u001b[0m client: Address(host='127.0.0.1', port=54146)\n\u001b[32m2024-09-08 14:56:20\u001b[0m \u001b[35m787c2bb17e6d\u001b[0m \u001b[34mutils.model[144]\u001b[0m \u001b[1;30mWARNING\u001b[0m \u001b[33mRole 'system' of this model is not supported, ignored.\u001b[0m\n\u001b[32m2024-09-08 14:56:20\u001b[0m \u001b[35m787c2bb17e6d\u001b[0m \u001b[34mapi.openai.v1.chat[144]\u001b[0m \u001b[1;30mINFO\u001b[0m translate: <reserved_106>将下面的日文文本翻译成中文：国境の長いトンネルを抜けると雪国であった。夜の底が白くなった。信号所に汽車が止まった。<reserved_107>\nSetting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n\u001b[32m2024-09-08 14:57:08\u001b[0m \u001b[35m787c2bb17e6d\u001b[0m \u001b[34mutils.model[144]\u001b[0m \u001b[1;30mINFO\u001b[0m Output generated in 48.54 seconds (0.58 tokens/s, 28 tokens, context 52 tokens)\n\u001b[32m2024-09-08 14:57:08\u001b[0m \u001b[35m787c2bb17e6d\u001b[0m \u001b[34mapi.openai.v1.chat[144]\u001b[0m \u001b[1;30mINFO\u001b[0m answer: prompt_token=52 new_token=28 text='穿过国境的漫长隧道后，我们来到雪国。夜晚的底部变得一片雪白。火车在信号所停了下来。' finish_reason='stop'\n\u001b[32m2024-09-08 14:59:56\u001b[0m \u001b[35m787c2bb17e6d\u001b[0m \u001b[34mapi[144]\u001b[0m \u001b[1;30mINFO\u001b[0m Incoming request api: http://31a4-35-188-128-107.ngrok-free.app/v1/chat/completions\n\u001b[32m2024-09-08 14:59:56\u001b[0m \u001b[35m787c2bb17e6d\u001b[0m \u001b[34mapi[144]\u001b[0m \u001b[1;30mINFO\u001b[0m client: Address(host='127.0.0.1', port=44934)\n\u001b[32m2024-09-08 14:59:56\u001b[0m \u001b[35m787c2bb17e6d\u001b[0m \u001b[34mutils.model[144]\u001b[0m \u001b[1;30mWARNING\u001b[0m \u001b[33mRole 'system' of this model is not supported, ignored.\u001b[0m\n\u001b[32m2024-09-08 14:59:56\u001b[0m \u001b[35m787c2bb17e6d\u001b[0m \u001b[34mapi.openai.v1.chat[144]\u001b[0m \u001b[1;30mINFO\u001b[0m translate: <reserved_106>将下面的日文文本翻译成中文：サポートキャスト<reserved_107>\nSetting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n\u001b[32m2024-09-08 14:59:59\u001b[0m \u001b[35m787c2bb17e6d\u001b[0m \u001b[34mutils.model[144]\u001b[0m \u001b[1;30mINFO\u001b[0m Output generated in 3.50 seconds (0.57 tokens/s, 2 tokens, context 19 tokens)\n\u001b[32m2024-09-08 14:59:59\u001b[0m \u001b[35m787c2bb17e6d\u001b[0m \u001b[34mapi.openai.v1.chat[144]\u001b[0m \u001b[1;30mINFO\u001b[0m answer: prompt_token=19 new_token=2 text='配角' finish_reason='stop'\n\u001b[32m2024-09-08 15:00:48\u001b[0m \u001b[35m787c2bb17e6d\u001b[0m \u001b[34mapi[144]\u001b[0m \u001b[1;30mINFO\u001b[0m Incoming request api: http://31a4-35-188-128-107.ngrok-free.app/v1/chat/completions?do_sample=true&num_beams=1&repetition_penalty=1.0\n\u001b[32m2024-09-08 15:00:48\u001b[0m \u001b[35m787c2bb17e6d\u001b[0m \u001b[34mapi[144]\u001b[0m \u001b[1;30mINFO\u001b[0m client: Address(host='127.0.0.1', port=50440)\n\u001b[32m2024-09-08 15:00:48\u001b[0m \u001b[35m787c2bb17e6d\u001b[0m \u001b[34mutils.model[144]\u001b[0m \u001b[1;30mWARNING\u001b[0m \u001b[33mRole 'system' of this model is not supported, ignored.\u001b[0m\n\u001b[32m2024-09-08 15:00:48\u001b[0m \u001b[35m787c2bb17e6d\u001b[0m \u001b[34mapi.openai.v1.chat[144]\u001b[0m \u001b[1;30mINFO\u001b[0m translate: <reserved_106>将下面的日文文本翻译成中文：幹部\nオープニング\n出口\nフラグアイコン1\nはい\nいいえ\n入る\n戻る\n表情：通常\n表情：目そらし\n表情：苦笑\nポーズ：スケベ顔\n表情：微笑み\n表情：引きつる\n表情：諦める\nポーズ：考える\n次へ\nポーズ：悔しがる\n表情：我慢\n表情：焦る<reserved_107>\nSetting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n\u001b[32m2024-09-08 15:00:48\u001b[0m \u001b[35m787c2bb17e6d\u001b[0m \u001b[34mapi[144]\u001b[0m \u001b[1;30mINFO\u001b[0m Incoming request api: http://31a4-35-188-128-107.ngrok-free.app/v1/chat/completions?do_sample=true&num_beams=1&repetition_penalty=1.0\n\u001b[32m2024-09-08 15:00:48\u001b[0m \u001b[35m787c2bb17e6d\u001b[0m \u001b[34mapi[144]\u001b[0m \u001b[1;30mINFO\u001b[0m client: Address(host='127.0.0.1', port=50448)\n\u001b[32m2024-09-08 15:00:48\u001b[0m \u001b[35m787c2bb17e6d\u001b[0m \u001b[34mutils.model[144]\u001b[0m \u001b[1;30mWARNING\u001b[0m \u001b[33mRole 'system' of this model is not supported, ignored.\u001b[0m\n\u001b[32m2024-09-08 15:00:48\u001b[0m \u001b[35m787c2bb17e6d\u001b[0m \u001b[34mapi.openai.v1.chat[144]\u001b[0m \u001b[1;30mINFO\u001b[0m translate: <reserved_106>将下面的日文文本翻译成中文：ハイキック\n乳首つねり1\n乳首つねり2\n乳首つねり3\n乳首つねり4\n表情：照れる\n表情：呆れる\n表情：不満\n母胎：赤毛短髪\n母胎：茶髪ロング\nいまこん\n感電1\n感電2\n感電後\n称号\n二つ名\n年齢\n性別\n身長\n体重<reserved_107>\n\u001b[32m2024-09-08 15:00:49\u001b[0m \u001b[35m787c2bb17e6d\u001b[0m \u001b[34mapi[144]\u001b[0m \u001b[1;30mINFO\u001b[0m Incoming request api: http://31a4-35-188-128-107.ngrok-free.app/v1/chat/completions?do_sample=true&num_beams=1&repetition_penalty=1.0\n\u001b[32m2024-09-08 15:00:49\u001b[0m \u001b[35m787c2bb17e6d\u001b[0m \u001b[34mapi[144]\u001b[0m \u001b[1;30mINFO\u001b[0m client: Address(host='127.0.0.1', port=50456)\n\u001b[32m2024-09-08 15:00:49\u001b[0m \u001b[35m787c2bb17e6d\u001b[0m \u001b[34mutils.model[144]\u001b[0m \u001b[1;30mWARNING\u001b[0m \u001b[33mRole 'system' of this model is not supported, ignored.\u001b[0m\n\u001b[32m2024-09-08 15:00:49\u001b[0m \u001b[35m787c2bb17e6d\u001b[0m \u001b[34mapi.openai.v1.chat[144]\u001b[0m \u001b[1;30mINFO\u001b[0m translate: <reserved_106>将下面的日文文本翻译成中文：表情：発情3\n表情：発情4\n表情：スケベ顔2\n表情：快感\n表情：顔射1\n表情：顔射2\n乳揉み1\n乳揉み2\n尻揉み1\n尻揉み2\n乳揉み3\n乳揉み4\n尻揉み3\n尻揉み4\n表情：泣く\nポーズ：ハイキック2\n乳揉み5\nカンチョー1\nカンチョー2\nカンチョー3<reserved_107>\n\u001b[32m2024-09-08 15:00:49\u001b[0m \u001b[35m787c2bb17e6d\u001b[0m \u001b[34mapi[144]\u001b[0m \u001b[1;30mINFO\u001b[0m Incoming request api: http://31a4-35-188-128-107.ngrok-free.app/v1/chat/completions?do_sample=true&num_beams=1&repetition_penalty=1.0\n\u001b[32m2024-09-08 15:00:49\u001b[0m \u001b[35m787c2bb17e6d\u001b[0m \u001b[34mapi[144]\u001b[0m \u001b[1;30mINFO\u001b[0m client: Address(host='127.0.0.1', port=50472)\n\u001b[32m2024-09-08 15:00:49\u001b[0m \u001b[35m787c2bb17e6d\u001b[0m \u001b[34mutils.model[144]\u001b[0m \u001b[1;30mWARNING\u001b[0m \u001b[33mRole 'system' of this model is not supported, ignored.\u001b[0m\n\u001b[32m2024-09-08 15:00:49\u001b[0m \u001b[35m787c2bb17e6d\u001b[0m \u001b[34mapi.openai.v1.chat[144]\u001b[0m \u001b[1;30mINFO\u001b[0m translate: <reserved_106>将下面的日文文本翻译成中文：攻撃\n防御\nアイテム\nスキル\n装備\nステータス\n並び替え\nセーブ\nゲーム終了\nオプション\n防具\n大事なもの\n最強装備\n全て外す\nニューゲーム\nコンティニュー\nタイトルへ\nやめる\n購入する\n売却する<reserved_107>\n\u001b[32m2024-09-08 15:00:49\u001b[0m \u001b[35m787c2bb17e6d\u001b[0m \u001b[34mapi[144]\u001b[0m \u001b[1;30mINFO\u001b[0m Incoming request api: http://31a4-35-188-128-107.ngrok-free.app/v1/chat/completions?do_sample=true&num_beams=1&repetition_penalty=1.0\n\u001b[32m2024-09-08 15:00:49\u001b[0m \u001b[35m787c2bb17e6d\u001b[0m \u001b[34mapi[144]\u001b[0m \u001b[1;30mINFO\u001b[0m client: Address(host='127.0.0.1', port=50488)\n\u001b[32m2024-09-08 15:00:49\u001b[0m \u001b[35m787c2bb17e6d\u001b[0m \u001b[34mutils.model[144]\u001b[0m \u001b[1;30mWARNING\u001b[0m \u001b[33mRole 'system' of this model is not supported, ignored.\u001b[0m\n\u001b[32m2024-09-08 15:00:49\u001b[0m \u001b[35m787c2bb17e6d\u001b[0m \u001b[34mapi.openai.v1.chat[144]\u001b[0m \u001b[1;30mINFO\u001b[0m translate: <reserved_106>将下面的日文文本翻译成中文：イエロー\n戦闘員♂\n戦闘員♀A\n戦闘員♀B\n戦闘員♀C\n戦闘員♀D\n戦闘員♀E\n戦闘員♀F\n研究員\n市民♂\n市民♀\n市民子ども\n帽子\n指輪\nこうもり\nスライム\nオーク\nミノタウロス\nポーション\nマジックウォーター<reserved_107>\n\u001b[32m2024-09-08 15:00:49\u001b[0m \u001b[35m787c2bb17e6d\u001b[0m \u001b[34mapi[144]\u001b[0m \u001b[1;30mINFO\u001b[0m Incoming request api: http://31a4-35-188-128-107.ngrok-free.app/v1/chat/completions?do_sample=true&num_beams=1&repetition_penalty=1.0\n\u001b[32m2024-09-08 15:00:49\u001b[0m \u001b[35m787c2bb17e6d\u001b[0m \u001b[34mapi[144]\u001b[0m \u001b[1;30mINFO\u001b[0m client: Address(host='127.0.0.1', port=50498)\n\u001b[32m2024-09-08 15:00:49\u001b[0m \u001b[35m787c2bb17e6d\u001b[0m \u001b[34mutils.model[144]\u001b[0m \u001b[1;30mWARNING\u001b[0m \u001b[33mRole 'system' of this model is not supported, ignored.\u001b[0m\n\u001b[32m2024-09-08 15:00:49\u001b[0m \u001b[35m787c2bb17e6d\u001b[0m \u001b[34mapi.openai.v1.chat[144]\u001b[0m \u001b[1;30mINFO\u001b[0m translate: <reserved_106>将下面的日文文本翻译成中文：表情：ジト目\nポーズ：目そらし\n表情：笑顔\n表情：怒り\nポーズ：ハイキック\n全裸\n表情：気絶\n表情：困惑\n表情：驚く\n表情：スケベ顔\nポーズ：困惑\nポーズ：恥じらい\nポーズ：気合\n表情：悔しがる\nポーズ：背中1\nポーズ：背中2\nポーズ：背中3\nポーズ：背中4\n表情：発情1\n表情：発情2<reserved_107>\n\u001b[32m2024-09-08 15:00:49\u001b[0m \u001b[35m787c2bb17e6d\u001b[0m \u001b[34mapi[144]\u001b[0m \u001b[1;30mINFO\u001b[0m Incoming request api: http://31a4-35-188-128-107.ngrok-free.app/v1/chat/completions?do_sample=true&num_beams=1&repetition_penalty=1.0\n\u001b[32m2024-09-08 15:00:49\u001b[0m \u001b[35m787c2bb17e6d\u001b[0m \u001b[34mapi[144]\u001b[0m \u001b[1;30mINFO\u001b[0m client: Address(host='127.0.0.1', port=50504)\n\u001b[32m2024-09-08 15:00:49\u001b[0m \u001b[35m787c2bb17e6d\u001b[0m \u001b[34mutils.model[144]\u001b[0m \u001b[1;30mWARNING\u001b[0m \u001b[33mRole 'system' of this model is not supported, ignored.\u001b[0m\n\u001b[32m2024-09-08 15:00:49\u001b[0m \u001b[35m787c2bb17e6d\u001b[0m \u001b[34mapi.openai.v1.chat[144]\u001b[0m \u001b[1;30mINFO\u001b[0m translate: <reserved_106>将下面的日文文本翻译成中文：どのファイルをロードしますか？\nは魔法を打ち消した！\nは魔法を跳ね返した！\nME 音量\nを獲得！\nお金を\n手に入れた！\nを手に入れた！\nを覚えた！\nたち\n持っている数\nは先手を取った！\nどのファイルにセーブしますか？\nSE 音量\nをかばった！\nは不意をつかれた！\nを使った！\nの勝利！\nブルー\nレッド<reserved_107>\n\u001b[32m2024-09-08 15:00:49\u001b[0m \u001b[35m787c2bb17e6d\u001b[0m \u001b[34mapi[144]\u001b[0m \u001b[1;30mINFO\u001b[0m Incoming request api: http://31a4-35-188-128-107.ngrok-free.app/v1/chat/completions?do_sample=true&num_beams=1&repetition_penalty=1.0\n\u001b[32m2024-09-08 15:00:49\u001b[0m \u001b[35m787c2bb17e6d\u001b[0m \u001b[34mapi[144]\u001b[0m \u001b[1;30mINFO\u001b[0m client: Address(host='127.0.0.1', port=50508)\n\u001b[32m2024-09-08 15:00:49\u001b[0m \u001b[35m787c2bb17e6d\u001b[0m \u001b[34mutils.model[144]\u001b[0m \u001b[1;30mWARNING\u001b[0m \u001b[33mRole 'system' of this model is not supported, ignored.\u001b[0m\n\u001b[32m2024-09-08 15:00:49\u001b[0m \u001b[35m787c2bb17e6d\u001b[0m \u001b[34mapi.openai.v1.chat[144]\u001b[0m \u001b[1;30mINFO\u001b[0m translate: <reserved_106>将下面的日文文本翻译成中文：移動禁止\nフラグ1章\nフラグ2章01\nフラグ2章02\n幹部チップ\nチェイス\nCダウン\nC立ち\n資料ゲット\nカードキー\n短剣\nフレイル\nムチ\nクロスボウ\nグローブ\n戦隊戦姫バスターブルー-悪の組織への潜入-\nレベル\n経験値\n戦う\n逃げる<reserved_107>\n\u001b[32m2024-09-08 15:00:49\u001b[0m \u001b[35m787c2bb17e6d\u001b[0m \u001b[34mapi[144]\u001b[0m \u001b[1;30mINFO\u001b[0m Incoming request api: http://31a4-35-188-128-107.ngrok-free.app/v1/chat/completions?do_sample=true&num_beams=1&repetition_penalty=1.0\n\u001b[32m2024-09-08 15:00:49\u001b[0m \u001b[35m787c2bb17e6d\u001b[0m \u001b[34mapi[144]\u001b[0m \u001b[1;30mINFO\u001b[0m client: Address(host='127.0.0.1', port=50514)\n\u001b[32m2024-09-08 15:00:49\u001b[0m \u001b[35m787c2bb17e6d\u001b[0m \u001b[34mutils.model[144]\u001b[0m \u001b[1;30mWARNING\u001b[0m \u001b[33mRole 'system' of this model is not supported, ignored.\u001b[0m\n\u001b[32m2024-09-08 15:00:49\u001b[0m \u001b[35m787c2bb17e6d\u001b[0m \u001b[34mapi.openai.v1.chat[144]\u001b[0m \u001b[1;30mINFO\u001b[0m translate: <reserved_106>将下面的日文文本翻译成中文：が上がった！\nが元に戻った！\nコマンド記憶\nの反撃！\n痛恨の一撃！！\n会心の一撃！！\nが下がった！\nは戦いに敗れた。\nが出現！\nのダメージを与えた！\n奪った！\nにダメージを与えられない！\nしかし逃げることはできなかった！\nは逃げ出した！\nは攻撃をかわした！\n次の\nまで\n現在の\nファイル\nに上がった！<reserved_107>\n\u001b[32m2024-09-08 15:00:49\u001b[0m \u001b[35m787c2bb17e6d\u001b[0m \u001b[34mapi[144]\u001b[0m \u001b[1;30mINFO\u001b[0m Incoming request api: http://31a4-35-188-128-107.ngrok-free.app/v1/chat/completions?do_sample=true&num_beams=1&repetition_penalty=1.0\n\u001b[32m2024-09-08 15:00:49\u001b[0m \u001b[35m787c2bb17e6d\u001b[0m \u001b[34mapi[144]\u001b[0m \u001b[1;30mINFO\u001b[0m client: Address(host='127.0.0.1', port=50524)\n\u001b[32m2024-09-08 15:00:49\u001b[0m \u001b[35m787c2bb17e6d\u001b[0m \u001b[34mutils.model[144]\u001b[0m \u001b[1;30mWARNING\u001b[0m \u001b[33mRole 'system' of this model is not supported, ignored.\u001b[0m\n\u001b[32m2024-09-08 15:00:49\u001b[0m \u001b[35m787c2bb17e6d\u001b[0m \u001b[34mapi.openai.v1.chat[144]\u001b[0m \u001b[1;30mINFO\u001b[0m translate: <reserved_106>将下面的日文文本翻译成中文：軽装防具\n重装防具\n小型盾\n大型盾\n物理\n武器\n身体\n装飾品\n魔法\n必殺技\n画面読込時\n衣服着脱状態\n服配布\n追尾\nスイッチ\nガス終了\n鉄格子\n雑談EF_淫乱度\n6章女戦闘員\nウェイト<reserved_107>\n\u001b[32m2024-09-08 15:00:50\u001b[0m \u001b[35m787c2bb17e6d\u001b[0m \u001b[34mapi[144]\u001b[0m \u001b[1;30mINFO\u001b[0m Incoming request api: http://31a4-35-188-128-107.ngrok-free.app/v1/chat/completions?do_sample=true&num_beams=1&repetition_penalty=1.0\n\u001b[32m2024-09-08 15:00:50\u001b[0m \u001b[35m787c2bb17e6d\u001b[0m \u001b[34mapi[144]\u001b[0m \u001b[1;30mINFO\u001b[0m client: Address(host='127.0.0.1', port=50530)\n\u001b[32m2024-09-08 15:00:50\u001b[0m \u001b[35m787c2bb17e6d\u001b[0m \u001b[34mutils.model[144]\u001b[0m \u001b[1;30mWARNING\u001b[0m \u001b[33mRole 'system' of this model is not supported, ignored.\u001b[0m\n\u001b[32m2024-09-08 15:00:50\u001b[0m \u001b[35m787c2bb17e6d\u001b[0m \u001b[34mapi.openai.v1.chat[144]\u001b[0m \u001b[1;30mINFO\u001b[0m translate: <reserved_106>将下面的日文文本翻译成中文：バスト\nウェスト\nヒップ\n衣装\nプロフィール\nコメント\n正気度\n♡正気度：\n淫乱度\n♡淫乱度：\n処女\n本名\n蒼木　サキ\nメニューの背景色\nバスターズ,2\nサイアーク\nキャラクター立絵\n以下ステータス\n一般防具\n魔法防具<reserved_107>\n\u001b[32m2024-09-08 15:00:50\u001b[0m \u001b[35m787c2bb17e6d\u001b[0m \u001b[34mapi[144]\u001b[0m \u001b[1;30mINFO\u001b[0m Incoming request api: http://31a4-35-188-128-107.ngrok-free.app/v1/chat/completions?do_sample=true&num_beams=1&repetition_penalty=1.0\n\u001b[32m2024-09-08 15:00:50\u001b[0m \u001b[35m787c2bb17e6d\u001b[0m \u001b[34mapi[144]\u001b[0m \u001b[1;30mINFO\u001b[0m client: Address(host='127.0.0.1', port=50542)\n\u001b[32m2024-09-08 15:00:50\u001b[0m \u001b[35m787c2bb17e6d\u001b[0m \u001b[34mutils.model[144]\u001b[0m \u001b[1;30mWARNING\u001b[0m \u001b[33mRole 'system' of this model is not supported, ignored.\u001b[0m\n\u001b[32m2024-09-08 15:00:50\u001b[0m \u001b[35m787c2bb17e6d\u001b[0m \u001b[34mapi.openai.v1.chat[144]\u001b[0m \u001b[1;30mINFO\u001b[0m translate: <reserved_106>将下面的日文文本翻译成中文：最大ＨＰ\n最大ＭＰ\n攻撃力\n防御力\n魔法力\n魔法防御\n敏捷性\n命中率\n回避率\nには効かなかった！\nのダメージを受けた！\n奪われた！\n増えた！\n減った！\nはダメージを受けていない！\nミス！\n回復した！\n常時ダッシュ\nBGM 音量\nBGS 音量<reserved_107>\n\u001b[32m2024-09-08 15:02:55\u001b[0m \u001b[35m787c2bb17e6d\u001b[0m \u001b[34mutils.model[144]\u001b[0m \u001b[1;30mINFO\u001b[0m Output generated in 126.68 seconds (0.58 tokens/s, 73 tokens, context 117 tokens)\n\u001b[32m2024-09-08 15:02:55\u001b[0m \u001b[35m787c2bb17e6d\u001b[0m \u001b[34mapi.openai.v1.chat[144]\u001b[0m \u001b[1;30mINFO\u001b[0m answer: prompt_token=117 new_token=73 text='干部\\n开场\\n出口\\n旗标图示1\\n是\\n不是\\n进入\\n返回\\n表情：普通\\n表情：左顾右盼\\n表情：苦笑\\n姿势：色狼脸\\n表情：微笑\\n表情：抽搐\\n表情：放弃\\n姿势：思考\\n下一页\\n姿势：懊悔\\n表情：忍耐\\n表情：焦急' finish_reason='stop'\nSetting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n\u001b[32m2024-09-08 15:02:56\u001b[0m \u001b[35m787c2bb17e6d\u001b[0m \u001b[34mapi[144]\u001b[0m \u001b[1;30mINFO\u001b[0m Incoming request api: http://31a4-35-188-128-107.ngrok-free.app/v1/chat/completions?do_sample=true&num_beams=1&repetition_penalty=1.0\n\u001b[32m2024-09-08 15:02:56\u001b[0m \u001b[35m787c2bb17e6d\u001b[0m \u001b[34mapi[144]\u001b[0m \u001b[1;30mINFO\u001b[0m client: Address(host='127.0.0.1', port=49544)\n\u001b[32m2024-09-08 15:02:56\u001b[0m \u001b[35m787c2bb17e6d\u001b[0m \u001b[34mutils.model[144]\u001b[0m \u001b[1;30mWARNING\u001b[0m \u001b[33mRole 'system' of this model is not supported, ignored.\u001b[0m\n\u001b[32m2024-09-08 15:02:56\u001b[0m \u001b[35m787c2bb17e6d\u001b[0m \u001b[34mapi.openai.v1.chat[144]\u001b[0m \u001b[1;30mINFO\u001b[0m translate: <reserved_106>将下面的日文文本翻译成中文：ディスペルハーブ\nスティミュラント\n戦闘服\n第1章\n集会場\nプロローグ\nスタート\n通路⓪\n入口\n裏路地\n控室\n通路①\n通路②\n控室_シナリオ\n第3章\n第2章\n通路③\n研究室\nガス室\n幹部部屋<reserved_107>\n\u001b[32m2024-09-08 15:04:57\u001b[0m \u001b[35m787c2bb17e6d\u001b[0m \u001b[34mutils.model[144]\u001b[0m \u001b[1;30mINFO\u001b[0m Output generated in 121.82 seconds (0.57 tokens/s, 70 tokens, context 112 tokens)\n\u001b[32m2024-09-08 15:04:57\u001b[0m \u001b[35m787c2bb17e6d\u001b[0m \u001b[34mapi.openai.v1.chat[144]\u001b[0m \u001b[1;30mINFO\u001b[0m answer: prompt_token=112 new_token=70 text='上段踢\\n捏乳头1\\n捏乳头2\\n捏乳头3\\n捏乳头4\\n表情：害羞\\n表情：傻眼\\n表情：不满\\n母胎：红发短发\\n母胎：褐发长发\\n现在\\n触电1\\n触电2\\n触电后\\n称号\\n绰号\\n年龄\\n性别\\n身高\\n体重' finish_reason='stop'\nSetting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n\u001b[32m2024-09-08 15:04:58\u001b[0m \u001b[35m787c2bb17e6d\u001b[0m \u001b[34mapi[144]\u001b[0m \u001b[1;30mINFO\u001b[0m Incoming request api: http://31a4-35-188-128-107.ngrok-free.app/v1/chat/completions?do_sample=true&num_beams=1&repetition_penalty=1.0\n\u001b[32m2024-09-08 15:04:58\u001b[0m \u001b[35m787c2bb17e6d\u001b[0m \u001b[34mapi[144]\u001b[0m \u001b[1;30mINFO\u001b[0m client: Address(host='127.0.0.1', port=59846)\n\u001b[32m2024-09-08 15:04:58\u001b[0m \u001b[35m787c2bb17e6d\u001b[0m \u001b[34mutils.model[144]\u001b[0m \u001b[1;30mWARNING\u001b[0m \u001b[33mRole 'system' of this model is not supported, ignored.\u001b[0m\n\u001b[32m2024-09-08 15:04:58\u001b[0m \u001b[35m787c2bb17e6d\u001b[0m \u001b[34mapi.openai.v1.chat[144]\u001b[0m \u001b[1;30mINFO\u001b[0m translate: <reserved_106>将下面的日文文本翻译成中文：改造室\nガス室_イベント1\nガス室_イベント2\n第4章\n第5章\n第6章\nエピローグ\n屋上\n暗黒空間\nデバッグルーム\n回想_敬礼手マン\n回想_パイズリフェラ\n回想_ガス部屋\n回想_人体改造\n回想_処女喪失\n回想_逆レイプ\n回想_エピローグ\nの攻撃！\nスキル1番は［攻撃］コマンドを＠選択したときに使用されます。\nは身を守っている。<reserved_107>\n","output_type":"stream"}]}]}